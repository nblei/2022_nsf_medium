Figure~\ref{fig:overview} shows our overall proposed approach. Most parts of this approach are within the orange-background ``Training'' oval in Figure~\ref{fig:overview}, which indicates that they are only needed for training, before the cycle-accurate simulation that constructs the overall simulated side channel signal begins.
We propose to implement this overall signal construction efficiently by first preparing signal snippets $S_b(t)$ for each type of event in each of the processor's microarchitectural building blocks, along with per-building-block weights ${\alpha}_b$ that specify how to scale the contribution of each block to the overall side channel signal.
Given these snippets and weights, we can obtain timestamps of individual events through cycle-accurate simulation, and then construct the overall signal by adding the appropriate snippet to the signal at each point in the signal's timeline where the corresponding event occurs.

\begin{wrapfigure}{r}{0.45\textwidth}
	\centering
	\includegraphics[width=0.45\columnwidth,trim={0in 2.65in 6.89in 0in},clip]{figure/overview.pdf}
	\vspace{-0.3in}
	\caption{Overview of the proposed approach.}
	\label{fig:overview}
\end{wrapfigure}
Our training constructs the signal snippets and their estimated weights through 1) circuit-level simulation of individual events in individual building blocks, 2) through decomposition of measured signals from an actual processor, or 3) through a synergistic combination of these methods. Finally, simulation-generated and measured signals can be compared for validation and, if calibration against a real processor is desired, discrepancies found during validation can be used to improve circuit-level models, signal snippets, and per-block weights such that future simulated results would match measured signals better.

To implement this approach, our proposed research consists of four primary research thrusts:

\begin{enumerate}[topsep=0pt,itemsep=0pt]
\item Discovering how to efficiently generate detailed event timestaps and use them to combine signal snippets into the overall side channel signal, along with per-block and per-code-region breakdowns and attribution that can help computer architects and software developers assess how microarchitectural and software changes affect the tradeoff between performance, power, and side channel leakage.
\item Discovering how to efficiently perform circuit-level simulation of caches, functional units, and other building blocks of the processor, to provide per-event timelines for voltage, current, and possibly other metrics that are needed to construct signal snippets for various side channels. along with other useful information such as how much the values of the metrics depend on data values.
\item Discovering new methods that produce per-event signal snippets for various side channels, by 1) transforming the outputs of circuit-level simulation, 2) decomposing measured signals, or 3) synergistically combining both methods, and also design of new methods for estimating per-block weights.
\item Discovering new methods to validate simulated signals against measurements, and to adjust weights, signal snippets, and the simulator's timing parameters to calibrate snippets, weights, and timing parameters to a real processor.
\end{enumerate}

In the rest of this section, we provide more detail on each of these research thrusts, along with preliminary results that demonstrate the feasibility of the proposed approaches and provide an excellent starting point for the proposed research in each thrust.

\subsection{Thrust 1: Construction of Side-Channel Signals Through Cycle-Accurate Simulation}

With the scaled per-event signal snippets from training, we can construct the side channel signal that corresponds to longer-term program execution. This signal construction uses a microarchitecture-level cycle-accurate simulation to determine the times at which events occur in each of the processor's block-level parts (ALU, decoder, reservation stations, cache, etc.), and these per-block event timestamps are then used to combine per-event signal snippets into an overall simulated signal.

One of the key research problems for this thrust will be to explore the tradeoff between the accuracy of the generated signal and the level of detail in the cycle-accurate simulation.
For example, in an L1 data cache we may lump all events (hit, lookup with miss, fill following a miss, etc.) into a single ``cache action''  category that is represented by a single snippet. This reduction in the level of detail would still allow the generated signal to capture the impact of the overall timeline of cache activity, e.g., having more prominent signals when executing code regions with a lot of cache accesses, having low-signal-activity periods when the cache is not used (due to a processor stall between a miss and the corresponding fill), etc. However, these events would need to be differentiated to account for differences between reads and writes, to account for a lookup-miss having to initiate an access in the next-level cache while hits do not, etc.

A similar tradeoff will be explored with respect to the impact of data values.
We will start with data-value-agnostic simulation, which can still model some attacks
that exploit which code is executed, such as secret-dependent control flow.
We will then add ability to obtain data-value statistics in cycle-accurate simulation,
and leverage the signal envelopes produced by our symbolic circuit-level simulation, to identify events (and blocks) whose signal snippets are affected the most by data values.
For such blocks/events, we will identify the nature of data-value dependency and construct data-value-dependent snippets, transformations, and weights to account for data-value dependencies in the signal. We will do this selectively (not for all events) because there is a tradeoff between accuracy and training/simulation time, so a given amount of accuracy improvement will come at a lower cost (training/simulation time) when only the most data-value-dependent signal snippets are refined in this way.

\subsubsection{Preliminary Results for Thrust 1}

Our preliminary results for this thrust are based on EMSim~\cite{Nader2020}, a first-of-its-kind cycle-accurate simulator that generates practically relevant EM side channel signals for program execution on a canonical 5-stage pipelined processor. This simulator,
implemented by PI Prvulovic, Co-PI Zajic, and several PhD students they have co-advised, demonstrates the feasibility of using event timings from microarchitectural simulation, together with signals snippets, to generate overall side channel signals.

In EMSim, each pipeline stage of the processor is modeled as a single block, and each instruction is treated as a separate event for each stage. The signal snippets all use the same shape, which is a mathematical function (rather than a measured or circuit-simulation-derived shape that will be used in the proposed approach).
The results we show here use an exponentially-decaying oscillation as the snippet shape,
which tends to match measurements well.
Differences among types of events are represented in EMSim only by having each event type have a separate scaling factor for this common snippet shape, and
these scaling factors are estimated using linear regression from signals that are measured on an FPGA-based implementation of this processor. Note that, although a more detailed RTL model of this processor was available, EMSim only uses the microarchitectural model in its simulation; the RTL was only used to implement the processor that was used to measure signals for finding scaling factors (training) and for evaluation of EMSim's accuracy.

EMSim combines scaled snippets into the overall simulated side channel signal (for program code that was not used to estimate the scaling factors). For this, it uses a microarchitecture-level simulator of the processor's pipeline.
In each cycle, for each stage of the processor, this simulator identifies
\begin{wrapfigure}{r}{0.45\textwidth}
	\centering
		\vspace{-0.15in}
	\includegraphics[width=0.45\columnwidth,clip]{figure/bench.pdf}
	\vspace{-0.3in}
	\caption{Measured (top) and EMSim-generated (bottom) EM signal.}
	\label{fig:bench}
	\vspace{-0.1in}
\end{wrapfigure}
 which instruction is in that stage in that cycle, applies the corresponding scaling factor to the
decaying-oscillation shape, and adds the scaled ``snippet'' to the overall signal at that point in time.
The resulting signal for one such simulation is shown, along with the measured signal for the same program run, in Figure~\ref{fig:bench}. We observe that, although the signals have some differences, most of the prominent features from the measured signal are also present in the simulator-generated signal.

Our sensitivity analysis for EMSim~\cite{Nader2020} has demonstrated the importance of modeling timing and microarchitectural behaviors, i.e., the need for cycle-accurate simulation of the microarchitecture rather than a simpler architecture-level emulation of the instruction stream.

\begin{wrapfigure}{r}{0.45\textwidth}
	\centering
		\vspace{-0.15in}
	\includegraphics[width=0.45\columnwidth,clip]{figure/stall2.pdf}
	\vspace{-0.3in}
	\caption{Comparison of measured (green) and simulated (black) EM signals with (top) and without (bottom) modeling pipeline stalls.}
	\label{fig:stall}
\end{wrapfigure}
To illustrate the need to account for microarchitectural events, Figure\ref{fig:stall} shows how a pipeline stall affects the signal. For illustration purposes, we increased the latency of a MUL instruction to eight cycles, and show a situation where the next instruction uses the result of a MUL. In the top part in Figure~\ref{fig:stall}, where the stall is correctly modeled in EMSim, the simulated signal has good agreement with the measured signal. However, in the bottom part in Figure~\ref{fig:stall} EMSim was modified to ignore this read-after-write dependence, resulting in not modeling the stall and, consequently, not having a good match to the measured signal.
Similar effects are observed when the simulation ignores other microarchitectural events and their timing, e.g. cache misses, branch mispredictions, etc.

In addition to its impact on \emph{timing}, we observed that a branch misprediction also has noticeable impact on the \emph{shape} of the side-channel signal. In the 5-stage processor that was used in EMSim, a branch misprediction causes a flush of the first two pipeline stages, which adds two cycles to the execution and \emph{also} changes which instructions are in which stage of the pipeline for the next few cycles.
\begin{wrapfigure}{r}{0.5\textwidth}
	\includegraphics[width=0.5\columnwidth,clip]{figure/mise2.pdf}
\vspace{-0.3in}
	\caption{Signal without (left) and with (right) a branch misprediction.}
	\label{fig:mis}
   \vspace{-0.1in}
\end{wrapfigure}
 This is shown in Figure~\ref{fig:mis}, along with how the change impacts the measured EM signal.
This effect would be even more pronounced for a more sophisticated processor, with more instructions flushed and more change in overlap among activity in different blocks.

Overall, even though EMSim had used a simple mathematical function as the snippet shape for all events, the EM side channel signal it constructs matches the measured EM side channel signal relatively well. EMSim was evaluated using synthetic benchmarks that were specifically designed to create various combinations of instructions in different pipeline stages. Using normalized cross-correlation as the accuracy metric, we found that EMSim achieves 94.1\% accuracy. While this is sufficient for many purposes, e.g., assessing vulnerability to simple power attacks (SPA) and simple EM attacks (SEMA) that extract secrets by differentiating between executing different parts of the program code (e.g. a secret-dependent if-then-else) and/or differential attacks (DPA, DEMA) that depend on large differences in values that are operated on, more accuracy is likely to be needed for simulated signals to be used to assess vulnerability to more sophisticated attacks that differentiate between data values that are similar to each other. We expect that the proposed use of per-event signal snippets from circuit-level simulation, with additional data-value-dependent considerations, will enable such uses of our simulation-generated signals. Additionally, our proposed work will enable signals to be generated not only for the (simplest) pipelined processors, but also to the much mode complex processors (deep pipelines, superscalar, out-of-order, etc.), by extending cycle-accurate simulators such as gem5~\cite{Binkert:2011:GS:2024716.2024718} and ESESC~\cite{esesc}.

\subsection{Thrust 2: Circuit-Level Simulation to get Per-Event Current/Voltage Timelines}

Even when a full circuit-level model for an entire processor is available, circuit-level simulation is not efficient enough to go through numerous program runs, with billions of processor cycles in each run, that would be needed to provide computer architects and software developers with actionable insights about the impact of microarchitectural and/or software changes on the tradeoff between performance, energy, and side-channel vulnerability. For example, PI Kumar's symbolic execution framework~\cite{cherupalli2017} currently uses detailed gate-level models of the microprocessor hardware, resulting in high-fidelity reproduction of metrics that are needed to predict the microprocessor's EM and power side-channel signals. However, this comes at a high computational cost when modeling entire-program execution -- an FFT implementation for a 16-bit MSP430 DSP takes over 3 hours, and this time would become several times longer for more complex processors and orders of magnitude longer for realistic runs of larger applications.

Instead, our proposed approach will raise the abstraction level of the whole-program simulation to the microarchitecture level (i.e., cycle-accurate simulation); it will use circuit-level simulation only during training, separately for each block of the processor, e.g., decoder, adder, multiplier, instruction scheduler, cache, etc. to produce, for each kind of activity in a block, how currents and voltages in that block change over time (i.e. $I_b(t)$ and $V_b(t)$ in Figure~\ref{fig:overview}) during that activity. Each of these circuit-level simulations will only model la few cycles, as it only needs to account for activity that corresponds to a single event. For example, for a data cache the events would be a lookup with a read-hit, lookup for a read-miss, fill on a read-miss, and read completion after a miss and fill, and the corresponding events for a write.

To improve accuracy of our overall signals, we will leverage the ability of the PI Kumar's existing circuit-level simulator to evaluate circuit activity using symbolic data. For a given time period, the current (and other properties) can be modeled not as a single curve, but rather as a time-changing distribution or as an  ``envelope'' that shows the possible range of values at each point in time. This presents us with a number of research challenges: how to express these distributions in the way that aids accuracy of the overall signals produced by cycle-accurate simulation, how to prevent path explosion when modeling blocks that may have significant changes in activity depending on the data (e.g., a symbolic address causing a hit or a miss in a cache), how to express aggregate data-dependent quantities as simplified symbolic expressions, to allow microarchitecture-level simulation to account for significant changes in aggregate activity within a block without resorting to modeling the block at a lower lever of abstraction, etc.

To help designers of highly secure microprocessors, i.e., processors which should have minimal information leakage via side-channels for {\tt any}program, we will also investigate a complementary approach, Property Driven Automatic Transformation (PDAT)~\cite{bleier21}, which analyzes net switching activity within a circuit for any inputs and performs model checking that allows processor designers to choose (or redesign) specific building blocks in a way that minimizes data-dependent changes in activity that would leak information. In essence, rather than accounting for data-dependent activity and expressing it in the signals provided to cycle-accurate simulation, this approach changes the hardware of the block to minimize such data-dependent activity, likely eliminating the need to account for data-dependent activity for that block.

Finally, while we expect that overall current and voltage will allow our side-channel-specific transformations to produce highly accurate signal snippets, more detailed outputs, e.g., separate $I_b(t)$ and $V_b(t)$ for different kinds of circuit elements, e.g., transistors, interconnect wires, power delivery wires, etc. may be needed to further improve accuracy, and we will investigate when such refinements are needed and how to implement them efficiently. For example, transistors that have the same geometry may have similar impact on side-channel-relevant properties (current, voltage, etc.), and thus may be represented in aggregate, but activity of larger transistors may need to be aggregated separately from smaller ones, to allow different scaling factors to be used for their activity when generating side channel snippets and/or accounting for data-dependent changes in the side channel signal.

\subsubsection{Preliminary Results for Thrust 2}

\begin{wrapfigure}{r}{0.4\linewidth}
\vspace{-0.15in}
\includegraphics[width=\linewidth]{./figure/Bespoke_Flow_Fig.pdf}
\vspace{-0.3in}
\caption{Hardware-software co-analysis used to generate bespoke microprocessors.}
\label{fig:bespoke}
\end{wrapfigure}
Co-PI Kumar's prior efforts to enable software developer to analyze their software for analog side channels have resulted in a framework which performs symbolic execution of a program on a detailed (gate-level) model of a microprocessor~\cite{cherupalli2017}.
This framework was originally developed to automate design of ``bespoke microprocessors'' --- microprocessors designed to execute a single application binary.
The flowchart for this use case is in shown in Figure~\ref{fig:bespoke}, and the symbolic execution of the program on the hardware occurs in the ``Gate Activity Analysis'' stage.

This framework accounts for how activity within the processor is affected by program inputs (e.g. sensor inputs in an IoT system), both for data-dependent (data values) and instruction-dependent (which instructions end up being executed) activity within the circuitry of the processor. This symbolic execution framework can identify whether any possible execution of a program leads to significant changes in gate-level activity and, consequently, side-channel leakage, allowing programmers to add or remove often-expensive software mitigation techniques depending on which mitigation is actually needed. This framework has shown applicability for security applications~\cite{cherupalli20172}, including
taint-tracking for information flow security to locate information policy
violations, even violations which result from covert timing side-channels.

This prior work provides us with an excellent starting point for the proposed symbolic circuit-level simulation. However, as pointed out earlier, circuit-level simulation of an entire processor throughout the execution of a program is exceedingly slow compared to state-of-the-art microarchitecture-level (cycle-accurate) simulation. Our proposed approach will synergistically combine the advantages of symbolic circuit-level simulation and microarchitecture-level simulation, using circuit-level simulation only during training to generate accurate per-event signal snippets, and using the (orders of magnitude faster) microarchitecture-level simulation to model the entire microprocessor for an entire program execution and generate a side channel signal by combining pre-made snippets.

Additionally, while gate-level switching activity can be a good proxy for side channel information leakage, PI Prvulovic and Co-PI Zajic's past work~\cite{Nader2020} has shown that the mapping from gate-level activity to side channel signals tends to be proportional only for gates within a relatively homogeneous block (e.g., a cache, a multiplier, etc.), but the scaling factors for different blocks can differ significantly. Thus our proposed approach is to account for these unequal transformations on a block-by-block basis, accounting for the specifics of how each side channel (EM, power, etc.) is affected by gate-level events, to produce {\em side channel signals} (rather than proxy metrics) that can be compared to and calibrated against measured signals for various side channels.

\subsection{Thrust 3: Per-Event Side Channel Snippet Construction and Scaling}
\label{sec:proposed-transform}

This research thrust will explore 1) how to transform the results of the circuit-level simulation into a signal snippet that represents the shape of the side channel signal that is contributed to the overall side channel signal by an occurrence of that event, 2) how to estimate the magnitude and (at least to some extent) the shape of these side channel signal snippets by decomposing measured signals, and 3) how to synergistically combine the side channel snippets obtained from circuit-level simulation and from signal decomposition.

Specifically, after the circuit-level simulator produces $I_b(t)$ and $V_b(t)$ signals for a specific event that happens in a specific block, these need to be transformed into a side-channel signal snippet $S_b(t)$. This step accounts for how current and voltage (and their changes over time) impact that side-channel. For example, for the power side channel the transformed signal would be the product of the current and voltage signals, for the EM side channel the transformed signals would be using Maxwell' equations to produce transformed signals that are proportional to the first derivative of the current and voltage signals, etc.

Snippets $S_b(t)$ can also be obtained from measurements of side channel signals on a real processor. This can be used when a circuit-level model of some part of the processor is not available. Measurements can also be used to correct (or calibrate) the scale of snippets from circuit-level simulations that have systematic biases, e.g. due to imprecisions in transistor models or lack of accounting for some electrical, electromagnetic, or quantum effects during circuit-level simulations.

Our initial approach to get per-event snippets from measured data will be to collect side channel signals as the processor executes various microbenchmarks that are designed to create specific events in specific parts of the processor, allowing the measured signals to be decomposed (e.g., using SVD) into signal snippets that correspond to contributions of individual events.

Finally, we note that the two methods of obtaining per-event signal snippets are highly complementary. Specifically, because of measurements noise and bandwidth limitations, measurement-derived snippets tend to be very accurate in terms of the overall magnitude of the event's contribution to the signal~\cite{Callan:2014:PMM:2742155.2742179}, and less accurate in terms of the shape of that contribution (snippet) over the event's ``life span''. Conversely, circuit-level models are likely to produce very accurate shapes (if N identical-in-size transistors toggle, the current draw is proportional to N), with a lot of detail and metadata (e.g., the distribution or the ``envelope'' that shows how the signal snippet can change due to different data values). However, the magnitude of these shapes may not be scaled correctly due to imperfect modeling of device physics, device parameters, or the circuit. This means that, when both circuit-level models and empirical measurements are available, the model-generated signal snippets can be scaled to match the average magnitude of measurement-derived signals. Our initial approach for this will be to estimate the per-block scaling factors ($\alpha_b$ in Figure~\ref{fig:overview}) using linear regression, and we will refine this as needed to improve accuracy further (e.g., using functions other than simple multiplication with a constant scaling factor).

The second reason for using scaling factors (or other ``scaling'' functions) is to account for side-channel specific differences in attenuation of signals from different blocks. Specifically, for the EM side channel, signal contributions from different blocks may experience different propagation loss, depending on the materials each signal propagates through and/or the position of the antenna or probe. These differences in propagation can also be accounted for using the per-block scaling factors, which can be determined using either measured signals or simulation-based propagation models.

Finally, we will account for data-dependent activity by considering how the signal is affected by values both within each microarchitectural block and between the blocks. Fundamentally, signals in most side channels (e.g., EM and power) are created during  \textit{bit-flips} at the transistor-level~\cite{VANECK1985269,6766222}, i.e., by changes in values at outputs of individual gates, flip-flops, and entire building blocks. Values that are outputs of individual blocks (e.g., the result produced by an ALU or the value read from a data cache) are typically already tracked during cycle-accurate simulation, so we can account for how changes in these values affect the side channel by adjusting the scaling factors according to data-dependent metrics such as the number of bit-flips. Values within a building block (e.g., values of carry bits within an adder or values produced by individual stages within a multiplier), however, are typically not tracked by a cycle-accurate simulator, and doing so precisely would be very costly in terms of simulation time. However, we note that values at outputs of building blocks tend to drive long interconnect wiring, whereas the wiring that carries values within a block tends to be local. This implies that for most analog side channels, e.g., power, EM, etc., values at outputs of building blocks will tend to have a significantly stronger impact of the overall side channel signal, and this is also supported by our preliminary results. Therefore, bit-flips within a building blocks are likely to have an aggregate effect, i.e., the side channel signal is affected mostly when data values at inputs of the block produce large differences in the amount of internal bit-toggling. Rather than add simulation of such intra-block activity to the cycle-accurate simulator, we plan to account for this in the aggregate, by leveraging the ability of our circuit-level simulators to consider symbolic outputs and produce the envelope that describes how much the signal can vary depending on input values and, if such signal variation is significant enough, by generating symbolic expressions for how the aggregate amounts of internal switching depends on the input values of the block.

\subsubsection{Preliminary Results for Thrust 3}
\label{sec:prelim-transform}

Our preliminary results for this thrust are also based on our work on EMSim~\cite{Nader2020}, where we considered two main contributors to the EM side-channel signal emanated by each stage (in EMSim, an entire pipeline stage of the simple 5-stage processor is treated as a building block). The first group of contributors, which we call \textit{instruction-dependent} contributors, consists of per-use signal contribution of each micro-architectural block, e.g., whether instruction fetch is happening or not (due to a stall) in a particular cycle, whether or not the register-file is being written in the last stage of the pipeline in a particular cycle, etc.

The second group of contributors is \textit{data-dependent} activity, which is created by bit-flips on the long wires, such as address and data lines going to/from the cache, and in the processor's registers. In reality, operand and data values also change the signal in other parts of the processor, e.g., on wiring between gates within an ALU, but EMSim only accounted for values that are already available during cycle-accurate simulation, whereas accounting for other bit-flips would have required RTL-level or circuit-level modeling.

For both types of contributors, we assumed that activity creates a signal that has the same overall shape (an exponentially-decaying sinusoid), because we did not use circuit-level models to get a better estimate of the shape. However, the magnitude of that shape is estimated separately, using measured data, for each contributor. For instruction-dependent contributors, i.e., effect on the signal of a specific type of instruction utilizing a specific pipeline stage, this estimation relies on executing microbenchmarks in which a single instruction, surrounded by NOP instructions, is executed, with all of its operands and data values set to zero,
to minimize the effect of data-dependent contributors and create a \emph{baseline} level of activity for each type of instruction in each stage of the processor.

\begin{wrapfigure}{r}{0.45\textwidth}
	\centering
\vspace{-0.2in}
	\includegraphics[width=0.45\columnwidth,clip]{figure/amp2.pdf}
\vspace{-0.3in}
	\caption{Measured (green) and simulated (black) signal when each pipeline stage is treated a separate source (top), and when considering the entire processor as a single source (bottom). Red ellipses point out the largest differences between simulated and measured signals.}
	\label{fig:amp}
\end{wrapfigure}
Figure~\ref{fig:amp} shows the measured and EMSim-generated EM signals when an {\tt ADD} instruction progresses through the pipeline while all the other instructions are {\tt NOP}s. The top part of Figure~\ref{fig:amp} corresponds to EMSim simulation where the contribution of each stage is scaled separately, using the per-stage scaling factors obtained from measured data. The bottom part of Figure~\ref{fig:amp} shows the simulated signal when all stages are treated as one source, i.e. only one (average) scaling factor is estimated from measurements and then used for activity in any stage. As we can see in Figure~\ref{fig:amp}, failing to model each stage individually (as used in previous work~\cite{McCann:2017:TPT:3241189.3241207}) can lead to significant inaccuracy.

For data-dependent contributors, the pipeline simulator used in EMSim identifies the number of bit-flips on address and data lines and in the register being written. Intuitively, more bit-flips cause more current flow and thus larger amplitude of EM emanations. To combine the effects of such data-dependent activity with the baseline (instruction-dependent) activity, we define the \textit{activity-factor}, $beta$, as a \textbf{\textit{scaling factor}} for each baseline activity $A$. To find this $\beta$, we assume all bit-flips within the stage contribute equally, i.e. that each bit-flip contributes equally to the signal amplitude. Because the baseline signal amplitude already includes some bit-flips (instruction addresses and instructions themselves are non-zero when accessing the instruction cache, etc.), we calculate $\beta$ as $\beta = 1 + \frac{(\mathit{flips}_{new} - \mathit{flips}_{base})}{\mathit{flips}_{total}}$, where $\mathit{flips}_{new}$ is the total number of flips for the current instruction,  $\mathit{flips}_{base}$ is the total number of flips when the previous instruction is a {\tt NOP}, and $\mathit{flips}_{total}$ is the maximum possible number of flips for the current instruction. The overall scaling factor for the (decaying sinusoid) snippet is then $A' = \beta\times A$.

\begin{wrapfigure}{r}{0.45\textwidth}
	\centering
\vspace{-0.2in}
	\includegraphics[width=0.45\columnwidth]{figure/alpha2.pdf}
\vspace{-0.3in}
	\caption{Measured (green) and simulated (black) signal when activity factor is modeled using a linear regression model (top) and when an \textit{average} activity factor is used (bottom).}
	\label{fig:alpha}
\end{wrapfigure}
To illustrate why these data-dependent considerations also need to be considered separately for each block, Figure~\ref{fig:alpha} shows the measured and EMSim-generated signals, with the top part showing simulated signals that use per-stage activity factors in each cycle as described above. In the bottom part, the simulation always uses one (average) activity factor. We observe that accounting for data-dependent activity in at lest some parts of the processor is needed to achieve good accuracy of the simulated signal. Further analysis of measured data has also shown that not all bit-flips have the same effect on the signal. Specifically, we found that 1) bit-flips in the output of the ALU and the cache have the most significant impacts on the signal, which can be expected in this FPGA-based design because these outputs were the driving the longest total amount of interconnect to reach all of their fanout targets, and 2) bit-flips in about 64\% of block output bits had a negligible contribution to overall accuracy (as determined through stepwise regression~\cite{f-test}). This supports the approach described in this proposal: using measured data to estimate the contribution of each block (and how much of it is data-dependent), and then selectively add accounting for data-dependent activity only for blocks where such accounting can result in a meaningful improvement in accuracy. Also, our preliminary findings are encouraging because the largest data-dependent contributions were outputs of the largest units (ALU and cache), which implies that the most impactful bit-flips in larger processors are likely to occur at outputs of microarchitectural blocks -- values that are already modeled by cycle-accurate simulators -- while the values on wires within an individual block (connecting much smaller sub-blocks) are likely to have less of an effect. However, in our proposed work we plan to use aggregate accounting for toggling within blocks through symbolic circuit-level simulation to obtain not only the ``baseline'' signal snippets for each block, but also the ``envelopes'' and input-dependent expressions for how the shape of the signal snippet would be affected by the values at the block's inputs and, for register files, caches, etc., by changes to values stored within those blocks.

\subsection{Validation/Calibration Against Measured Signals}

In this thrust we will use measured side channel signals collected from real processors to
evaluate the accuracy of snippet generation, transformation, and overall simulated signals produced by the methods that will be devised. The results of this evaluation will help us identify likely sources of error and refine our approach to create efficient simulators that produce highly accurate side channel signals that are calibrated against specific real processors.

The insights from this evaluation, refinement, and calibration efforts will then be used to design systematic mechanisms that can automate this iterative refinement/calibration, resulting in mechanisms that selectively refine the circuit-level models, accounting for data-dependent behaviors, scaling factors, and other aspects of the training process. The goal here will be to achieve a desired level of accuracy while minimizing the impact of these refinements on simulation time, and to avoid costly refinements (e.g., development of more detailed circuit-level models) that do not lead to commensurate increases in signal accuracy.

\subsubsection{Preliminary Results for Thrust 4}

PIs Prvulovic and Zajic have extensive experience in side-channel measurements for EM, power, and several other analog side channels (acoustic, optical, temperature, ...), comparing these signals to each other, and, more recently, comparing synthesized or modeled signals to measured ones~\cite{Nader2020} and using the results of these measurements to refine the models.

PI Kumar in experienced in digital design, chip design,
tape-out, and testing, and runs an undergraduate course in which students design processors that are then are manufactured by TSMC and tested on the UIUC campus.
His group has also participated in Intel's Chip Design Challenge, manufacturing RISC-V microprocessors in Intel's 16 nm technology. This gives us a sizeable collection of microprocessor artifacts, with both large and small microarchitectural differences,
which can be subjected to side-channel measurement and also modeled at various levels of abstraction -- high-level microarchitecture, RTL, gate-level,
GDS/geometry level, and physical chips. This will allow us to evaluate the predictive accuracy of the simulation-generated signals with respect to microarchitectural changes,
and also to calibrate based on numerous real-world designs and implementations.

